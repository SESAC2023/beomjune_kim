{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chQTf0iKLHOt",
        "outputId": "891bb58f-5df6-42eb-d778-368de364e437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "dTvpSGI4Lvj7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 불러오기\n",
        "df = pd.read_csv('joongang.csv')\n",
        "df['text'] = df['title'] + ' ' + df['content']\n",
        "# NaN 값을 포함하는 행 삭제\n",
        "df.dropna(subset=['text'], inplace=True)"
      ],
      "metadata": {
        "id": "5ME_-mBtMDph"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 원본 데이터를 학습 데이터와 테스트 데이터로 나눔\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "DnxUXT0eS7af"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KoBERT 토크나이저 로드\n",
        "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')"
      ],
      "metadata": {
        "id": "EQOhdAQVTCBZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_data(texts, tokenizer, max_len=512):\n",
        "    all_tokens = []\n",
        "    all_masks = []\n",
        "\n",
        "    for text in texts:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_len,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # 직접 패딩 수행\n",
        "        padding_length = max_len - len(encoded_dict['input_ids'][0])\n",
        "        all_tokens.append(torch.cat([encoded_dict['input_ids'], torch.zeros((1, padding_length), dtype=torch.long)], dim=1))\n",
        "        all_masks.append(torch.cat([encoded_dict['attention_mask'], torch.zeros((1, padding_length), dtype=torch.long)], dim=1))\n",
        "\n",
        "    all_tokens = torch.cat(all_tokens, dim=0)\n",
        "    all_masks = torch.cat(all_masks, dim=0)\n",
        "\n",
        "    return all_tokens, all_masks"
      ],
      "metadata": {
        "id": "QYasfbwgLxpi"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 데이터 인코딩\n",
        "train_input_ids, train_attention_masks = encode_data(train_df['text'], tokenizer)\n",
        "train_labels2 = torch.tensor(train_df['label2'].values) - 1\n",
        "\n",
        "# 테스트 데이터 인코딩 및 성능 평가\n",
        "test_input_ids, test_attention_masks = encode_data(test_df['text'], tokenizer)\n",
        "test_labels2 = torch.tensor(test_df['label2'].values) - 1"
      ],
      "metadata": {
        "id": "6mkvTRPfL8RJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 및 데이터 로더 생성\n",
        "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels2)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels2)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False)"
      ],
      "metadata": {
        "id": "q0-a9xcHTXs4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# 모델 정의\n",
        "class KoBERTForSingleLabelClassification(torch.nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super(KoBERTForSingleLabelClassification, self).__init__()\n",
        "        self.config = BertConfig.from_pretrained('monologg/kobert', num_labels=num_labels, output_hidden_states=True)\n",
        "        self.bert = BertForSequenceClassification.from_pretrained('monologg/kobert', config=self.config)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        return outputs.logits\n",
        "\n",
        "\n",
        "model = KoBERTForSingleLabelClassification(df['label2'].nunique()).to(device)\n",
        "optimizer = Adam(model.parameters(), lr=1e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUk9iR7tL-H6",
        "outputId": "ffc66537-61d6-40a8-81f1-e2e5de21f47e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 데이터를 학습 및 검증 데이터로 나눔\n",
        "train_inputs, val_inputs, train_masks, val_masks, train_labels2, val_labels2 = train_test_split(\n",
        "    train_input_ids, train_attention_masks, train_labels2, random_state=42, test_size=0.1\n",
        ")\n",
        "\n",
        "train_dataset = TensorDataset(train_inputs, train_masks, train_labels2)\n",
        "val_dataset = TensorDataset(val_inputs, val_masks, val_labels2)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)"
      ],
      "metadata": {
        "id": "MriJKmpIMpUk"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "train_losses = []\n",
        "best_accuracy = 0\n",
        "best_model_path = \"best_model_label2.pth\"\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in tqdm(val_dataloader, desc=f\"Training Epoch {epoch+1}/{EPOCHS}\"):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, attention_mask, label = [b.to(device) for b in batch]\n",
        "        logits = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        loss = loss_fn(logits, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_correct += (logits.argmax(dim=1) == label).sum().item()\n",
        "        total_samples += label.size(0)\n",
        "\n",
        "    avg_train_loss = total_loss / total_samples\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    val_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids, attention_mask, label = [b.to(device) for b in batch]\n",
        "            logits = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            val_correct += (logits.argmax(dim=1) == label).sum().item()\n",
        "            val_samples += label.size(0)\n",
        "\n",
        "    val_accuracy = val_correct / val_samples\n",
        "\n",
        "    # 최고 정확도를 갱신할 경우 모델 저장\n",
        "    if val_accuracy > best_accuracy:\n",
        "        best_accuracy = val_accuracy\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Model saved for epoch {epoch+1} with accuracy: {best_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4EGXOjRPBhB",
        "outputId": "5c5b34df-7b44-45ed-b849-cec4068ff03d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/10: 100%|██████████| 10/10 [00:01<00:00,  7.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved for epoch 1 with accuracy: 0.4000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/10: 100%|██████████| 10/10 [00:01<00:00,  7.78it/s]\n",
            "Training Epoch 3/10: 100%|██████████| 10/10 [00:01<00:00,  7.74it/s]\n",
            "Training Epoch 4/10: 100%|██████████| 10/10 [00:01<00:00,  7.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved for epoch 4 with accuracy: 0.5100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5/10: 100%|██████████| 10/10 [00:01<00:00,  7.75it/s]\n",
            "Training Epoch 6/10: 100%|██████████| 10/10 [00:01<00:00,  7.76it/s]\n",
            "Training Epoch 7/10: 100%|██████████| 10/10 [00:01<00:00,  7.74it/s]\n",
            "Training Epoch 8/10: 100%|██████████| 10/10 [00:01<00:00,  7.73it/s]\n",
            "Training Epoch 9/10: 100%|██████████| 10/10 [00:01<00:00,  7.31it/s]\n",
            "Training Epoch 10/10: 100%|██████████| 10/10 [00:01<00:00,  7.76it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습이 끝난 후 가장 좋은 성능을 보인 모델을 불러옵니다.\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "model.eval()\n",
        "\n",
        "# 테스트 데이터셋에 대한 예측을 수행\n",
        "true_labels2 = []\n",
        "predicted_labels2 = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ids, attention_mask, label2 = [b.to(device) for b in batch]\n",
        "        logits1 = model(input_ids, attention_mask)\n",
        "\n",
        "        true_labels2.extend(label2.tolist())\n",
        "        predicted_labels2.extend(logits1.argmax(dim=1).tolist())\n",
        "\n",
        "\n",
        "# f1-score를 포함한 성능 지표를 출력\n",
        "accuracy2 = sum([1 if true == pred else 0 for true, pred in zip(true_labels2, predicted_labels2)]) / len(true_labels2)\n",
        "\n",
        "f1_2 = f1_score(true_labels2, predicted_labels2, average='weighted')\n",
        "\n",
        "\n",
        "print(f\"Test Accuracy for Label1: {accuracy2:.4f}\")\n",
        "print(f\"F1 Score for Label1: {f1_2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rI6F4hzPQE3l",
        "outputId": "65b35ce3-4da5-42ec-c9f6-731e76c39aa3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy for Label1: 0.5100\n",
            "F1 Score for Label1: 0.4674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_labels2))\n",
        "print(len(val_labels2))\n",
        "print(len(test_labels2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S12guPftW0sP",
        "outputId": "030cf548-4800-4bc0-8cdd-49b8a73d031d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "359\n",
            "40\n",
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nYT-6i4-XOI-"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}